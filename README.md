# ğŸ§  Private AI Document Assistant

A fully offline AI-powered assistant that helps you **ask natural language questions from your internal PDF documents**. Built for startups and teams that prioritize **data privacy**, **speed**, and **full control** â€” no cloud required.

---

## ğŸš€ Features

- ğŸ”’ **Private & Offline**: No API keys, no external servers â€” runs 100% locally
- ğŸ“„ **PDF Ingestion**: Upload or use preloaded documents from the `data/` folder
- ğŸ” **Smart Embeddings**: Uses `sentence-transformers` for accurate context search
- ğŸ§  **LLM Answering**: Uses Mistral 7B (or similar) via `llama-cpp-python`
- ğŸ—‚ï¸ **Chat History Logging**: Saves Q&A logs with timestamps
- ğŸ–¼ï¸ **Streamlit UI**: Intuitive, clean frontend for easy use
- ğŸ§± **Modular Structure**: Easy to extend and integrate

---

## ğŸ§  Tech Stack

| Component       | Tool / Library              |
|----------------|-----------------------------|
| Embeddings      | `sentence-transformers` (`all-MiniLM-L6-v2`) |
| Vector DB       | `ChromaDB` (local)          |
| PDF Parsing     | `pdfplumber`                |
| LLM             | `llama-cpp-python` + `mistral-7b.Q4_0.gguf` |
| Frontend        | `Streamlit`                 |
| Logging         | JSON-based log files        |

---

## ğŸ“ Folder Structure

```
private-ai-doc-assistant/
â”œâ”€â”€ app.py                      â† Main Streamlit app
â”œâ”€â”€ data/                       â† Drop your PDFs here
â”œâ”€â”€ embeddings/
â”‚   â””â”€â”€ chroma/                 â† Vector database folder
â”œâ”€â”€ models/
â”‚   â””â”€â”€ mistral-7b.Q4_0.gguf    â† Local LLM file
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ chat_engine.py          â† Handles retrieval + LLM response
â”‚   â”œâ”€â”€ chat_history.py         â† Saves interaction logs
â”‚   â””â”€â”€ upload_and_embed.py     â† Ingest + embed new PDFs (optional)
â”œâ”€â”€ chat_logs/                  â† Timestamped JSON Q&A logs
â”œâ”€â”€ README.md                   â† You're here
```

---

## ğŸ§ª How to Run

> ğŸ“ Prerequisite: Python 3.10+, and a machine capable of running a quantized model locally (e.g., Mistral or LLaMA)


1. **Install dependencies**
  ```bash
pip install -r requirements.txt

 Add your PDF files
  Drop your PDFs into the data/ folder

 Download a compatible LLM model
  Place a .gguf file (like mistral-7b.Q4_0.gguf) into the models/ folder
  You can get Mistral from huggingface.co/TheBloke

 Run the app
  streamlit run app.py

âœ¨ Sample Use Case
â€œWho are the target users of SmartAudit AI?â€
Answer: "The target users of SmartAudit AI are Fintech CTOs, auditors, and regulators."

ğŸ“„ License
This project is open source under the MIT License.

ğŸ™Œ Acknowledgements

- [Sentence Transformers](https://www.sbert.net/)
- [ChromaDB](https://www.trychroma.com/)
- [llama-cpp-python](https://github.com/abetlen/llama-cpp-python)
- [Streamlit](https://streamlit.io/)
