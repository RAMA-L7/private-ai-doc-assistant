# ðŸ§  Private AI Document Assistant

A fully offline AI-powered assistant that helps you **ask natural language questions from your internal PDF documents**. Built for startups and teams that prioritize **data privacy**, **speed**, and **full control** â€” no cloud required.

---

## ðŸš€ Features

- ðŸ”’ **Private & Offline**: No API keys, no external servers â€” runs 100% locally
- ðŸ“„ **PDF Ingestion**: Upload or use preloaded documents from the `data/` folder
- ðŸ” **Smart Embeddings**: Uses `sentence-transformers` for accurate context search
- ðŸ§  **LLM Answering**: Uses Mistral 7B (or similar) via `llama-cpp-python`
- ðŸ—‚ï¸ **Chat History Logging**: Saves Q&A logs with timestamps
- ðŸ–¼ï¸ **Streamlit UI**: Intuitive, clean frontend for easy use
- ðŸ§± **Modular Structure**: Easy to extend and integrate

---

## ðŸ§  Tech Stack

| Component       | Tool / Library              |
|----------------|-----------------------------|
| Embeddings      | `sentence-transformers` (`all-MiniLM-L6-v2`) |
| Vector DB       | `ChromaDB` (local)          |
| PDF Parsing     | `pdfplumber`                |
| LLM             | `llama-cpp-python` + `mistral-7b.Q4_0.gguf` |
| Frontend        | `Streamlit`                 |
| Logging         | JSON-based log files        |

---

## ðŸ“ Folder Structure

```
private-ai-doc-assistant/
â”œâ”€â”€ app.py                      â† Main Streamlit app
â”œâ”€â”€ data/                       â† Drop your PDFs here
â”œâ”€â”€ embeddings/
â”‚   â””â”€â”€ chroma/                 â† Vector database folder
â”œâ”€â”€ models/
â”‚   â””â”€â”€ mistral-7b.Q4_0.gguf    â† Local LLM file
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ chat_engine.py          â† Handles retrieval + LLM response
â”‚   â”œâ”€â”€ chat_history.py         â† Saves interaction logs
â”‚   â””â”€â”€ upload_and_embed.py     â† Ingest + embed new PDFs (optional)
â”œâ”€â”€ chat_logs/                  â† Timestamped JSON Q&A logs
â”œâ”€â”€ README.md                   â† You're here
```

---

## ðŸ§ª How to Run

> ðŸ“ Prerequisite: Python 3.10+, and a machine capable of running a quantized model locally (e.g., Mistral or LLaMA)


1. **Install dependencies**
  ```bash
pip install -r requirements.txt
```
 2. **Add your PDF files**
   - Drop your PDFs into the `data/` folder

3. **Download a compatible LLM model**
   - Place a `.gguf` file (like `mistral-7b.Q4_0.gguf`) into the `models/` folder  
   - You can get Mistral from [huggingface.co/TheBloke](https://huggingface.co/TheBloke)

4. **Run the app**
```bash
streamlit run app.py
```

---

 âœ¨ Sample Use Case

> _â€œWho are the target users of SmartAudit AI?â€_  
> Answer: *"The target users of SmartAudit AI are Fintech CTOs, auditors, and regulators."*

---

 ðŸ“„ License

This project is open source under the [MIT License](LICENSE).

---

 ðŸ™Œ Acknowledgements

- [Sentence Transformers](https://www.sbert.net/)
- [ChromaDB](https://www.trychroma.com/)
- [llama-cpp-python](https://github.com/abetlen/llama-cpp-python)
- [Streamlit](https://streamlit.io/)
